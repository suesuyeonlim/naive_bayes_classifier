{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Import YouTube Data & Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I import pre-pulled YouTube data containing titles, descriptions, and tags for more than 5,000 videos, and clean the data in the following ways to make it ready for training:\n",
    "\n",
    "1. Create a new field \"text\" which combines a video's title, description, and tags.\n",
    "2. Standardize the words in the text field by converting them to lower case and lemmatizing them.\n",
    "3. Limit the data to videos in English.\n",
    "4. Downsample from the majority class \"False\" for the target variable \"madeForKids\" to handle the class imbalance issue.\n",
    "5. Create a bag of words using the standardized text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the first step, I import various packages and YouTube data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google_auth_oauthlib.flow\n",
    "import googleapiclient.discovery\n",
    "import googleapiclient.errors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from langdetect import detect\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "data = pd.concat([pd.read_excel('Data/data1.xlsx', index_col = 0), \\\n",
    "                  pd.read_excel('Data/data2.xlsx', index_col = 0), \\\n",
    "                  pd.read_excel('Data/data3.xlsx', index_col = 0), \\\n",
    "                  pd.read_excel('Data/data4.xlsx', index_col = 0)], axis = 0).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing the YouTube data, I convert titles, descriptions, and tags to lowercase and concatenate them as a new field \"text\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['title'] + ' ' + data['description'].map(lambda x: ' '.join(x.split()) if x is not np.nan else '') + \\\n",
    "data['tags'].map(lambda x: ' '.join(x) if x is not np.nan else '')\n",
    "data['text'] = data['text'].map(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, I limit the data only to the videos in English, as that is the language that I want to focus on, and data in other languages could become noise for training models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-77ac0d62ca83>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['lang'][index] = detect(data['text'][index])\n",
      "<ipython-input-3-77ac0d62ca83>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['lang'][index] = 'other'\n"
     ]
    }
   ],
   "source": [
    "# Use the detect function of the package \"langdetect\"\n",
    "\n",
    "data['lang'] = None\n",
    "for index in data.index:\n",
    "    try:\n",
    "        data['lang'][index] = detect(data['text'][index])\n",
    "    except:\n",
    "        data['lang'][index] = 'other'\n",
    "# data = data[(data['lang'] == 'en')]        \n",
    "data = data[(data['lang'] == 'en') | (data['defaultAudioLanguage'].str.startswith(\"en\") == True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen below, there is a class imbalance issue. There are only 193 videos that were made for kids, whereas 3,831 videos were not made for them. For this reason, I downsample the data so that there is the same number of videos made for kids vs not made for kids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    4115\n",
       "True      212\n",
       "Name: madeForKids, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['madeForKids'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, I end up with 383 videos to train classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     212\n",
       "False    212\n",
       "Name: madeForKids, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "madeForKids = data[data['madeForKids'] == True]\n",
    "notMadeForKids = data[data[\"madeForKids\"] == False].sample(round(len(madeForKids) * 1.0))\n",
    "data_final = pd.concat([notMadeForKids, madeForKids], axis = 0)\n",
    "data_final['madeForKids'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step involves creating a bag of words, after cleaning words in text by 1) removing a set of commonly used words that do not have special meaning to indicate whether a video was made for kids or not, and 2) lemmatizing the words. Lemmatizing is a process of grouping inflected forms of words as single terms. For example, both \"loved\" and \"loves\" are grouped as the word \"love\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def bag_it(text):\n",
    "    bag = {}\n",
    "    for word in text.split(): \n",
    "        word_clean = lemmatizer.lemmatize(re.sub('\\W+','', word))\n",
    "        if word not in stop_words and word_clean != '' and word_clean.isalpha():\n",
    "            bag[word_clean] = bag.get(word_clean, 0) + 1\n",
    "    return bag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I create a bag of words for both the whole dataset and the sampled dataset. This is because, while I will use the sampled dataset for training, I plan to use the whole dataset as the final validationd dataset. A bag of words is a dataset with frequency for each word from a word universe for given example text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bag of words for the whole dataset\n",
    "\n",
    "docs = []\n",
    "for doc in data['text']:\n",
    "    docs.append(bag_it(doc))\n",
    "\n",
    "features = pd.DataFrame.from_dict(docs)\n",
    "features = features.replace(np.nan, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bag of words for the sampled dataset\n",
    "\n",
    "docs = []\n",
    "for doc in data_final['text']:\n",
    "    docs.append(bag_it(doc))\n",
    "\n",
    "features_final = pd.DataFrame.from_dict(docs)\n",
    "features_final = features_final.replace(np.nan, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Training Models to Predict Whether a Video was Made for Kids\n",
    "\n",
    "In this section, I train the following models and choose the best model to predict if a video was made for kids or not:\n",
    "\n",
    "1. Gradient Boosting\n",
    "2. AdaBoost\n",
    "3. Naive Bayes\n",
    "4. XGBoost\n",
    "\n",
    "In training the models, I use grid search to find the best paramters based on the accuracy score. In deciding which model performs the best, I use 1) accuracy score using training data; 2) accuracy score using test data; and 3) accuracy score using the whole dataset. \n",
    "\n",
    "Accuracy score using training data shows how well the model is able to capture information from the training data, and accuracy score using test data shows how well the model can be generalized to other data. Finally, accuracy score using the whole dataset shows how well the model predicts in the context of the actual distribution of the videos made for kids vs not made for kids. Specifically, I assess if a model can identify videos made for kids, while not flagging too many videos not made for kids, as made for kids, using confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the first step, I split the sampled data into a training dataset using (75% of the videos) and a test dataset (25% of the videos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features_final, data_final['madeForKids'], stratify = data_final['madeForKids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following steps, I train each model, and calculate accuracy scores using training data, test data, and the whole dataset. I also analyze the breakdown of the accuracy score for the whole dataset, and this step is important because we do not want to pick a model as the best one if accuracy score is high because it flags most videos as not made for kids (as it is the majority class), and does not really identify videos made for kids correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search found the following optimal parameters: \n",
      "learning_rate: 0.15\n",
      "\n",
      "Training Accuracy: 100.0%\n",
      "Confusion Matrix (True Negative / False Positive / False Negative / True Positive):\n",
      "[[159   0]\n",
      " [  0 159]]\n",
      "Validation Accuracy: 90.57%\n",
      "Confusion Matrix (True Negative / False Positive / False Negative / True Positive):\n",
      "[[49  4]\n",
      " [ 6 47]]\n",
      "Entire Data Accuracy: 93.27%\n",
      "Confusion Matrix (True Negative / False Positive / False Negative / True Positive):\n",
      "[[3830  285]\n",
      " [   6  206]]\n"
     ]
    }
   ],
   "source": [
    "# Model 1: Gradient Boosting\n",
    "\n",
    "gb = GradientBoostingClassifier()\n",
    "grid_gb = GridSearchCV(gb, {'learning_rate': [0.05, 0.1, 0.15]}, scoring='accuracy', cv = 5, n_jobs = 1)\n",
    "grid_gb.fit(X_train, y_train)\n",
    "\n",
    "best_parameters = grid_gb.best_params_\n",
    "\n",
    "print('Grid Search found the following optimal parameters: ')\n",
    "for param_name in sorted(best_parameters.keys()):\n",
    "    print('%s: %r' % (param_name, best_parameters[param_name]))\n",
    "\n",
    "training_accuracy = accuracy_score(y_train, grid_gb.predict(X_train))\n",
    "test_accuracy = accuracy_score(y_test, grid_gb.predict(X_test))\n",
    "entire_data_preds = grid_gb.predict(features[list(X_train.columns)])\n",
    "entire_data_accuracy = accuracy_score(data['madeForKids'], entire_data_preds)\n",
    "\n",
    "print('')\n",
    "print('Training Accuracy: {:.4}%'.format(training_accuracy * 100))\n",
    "print('Confusion Matrix (True Negative / False Positive / False Negative / True Positive):')\n",
    "print(confusion_matrix(y_train, grid_gb.predict(X_train)))\n",
    "print('Validation Accuracy: {:.4}%'.format(test_accuracy * 100))\n",
    "print('Confusion Matrix (True Negative / False Positive / False Negative / True Positive):')\n",
    "print(confusion_matrix(y_test, grid_gb.predict(X_test)))\n",
    "print('Entire Data Accuracy: {:.4}%'.format(entire_data_accuracy * 100))\n",
    "print('Confusion Matrix (True Negative / False Positive / False Negative / True Positive):')\n",
    "print(confusion_matrix(data['madeForKids'], entire_data_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search found the following optimal parameters: \n",
      "learning_rate: 0.35\n",
      "\n",
      "Training Accuracy: 92.77%\n",
      "Confusion Matrix (True Negative / False Positive / False Negative / True Positive):\n",
      "[[159   0]\n",
      " [ 23 136]]\n",
      "Validation accuracy: 86.79%\n",
      "Confusion Matrix (True Negative / False Positive / False Negative / True Positive):\n",
      "[[48  5]\n",
      " [ 9 44]]\n",
      "Entire Data Accuracy: 89.9%\n",
      "Confusion Matrix (True Negative / False Positive / False Negative / True Positive):\n",
      "[[3710  405]\n",
      " [  32  180]]\n"
     ]
    }
   ],
   "source": [
    "# Model 2: AdaBoost\n",
    "\n",
    "ab = AdaBoostClassifier()\n",
    "grid_ab = GridSearchCV(ab, {'learning_rate': [0.35, 0.4, 0.45]}, scoring='accuracy', cv = 5, n_jobs = 1)\n",
    "grid_ab.fit(X_train, y_train)\n",
    "\n",
    "best_parameters = grid_ab.best_params_\n",
    "\n",
    "print('Grid Search found the following optimal parameters: ')\n",
    "for param_name in sorted(best_parameters.keys()):\n",
    "    print('%s: %r' % (param_name, best_parameters[param_name]))\n",
    "\n",
    "training_accuracy = accuracy_score(y_train, grid_ab.predict(X_train))\n",
    "test_accuracy = accuracy_score(y_test, grid_ab.predict(X_test))\n",
    "entire_data_preds = grid_ab.predict(features[list(X_train.columns)])\n",
    "entire_data_accuracy = accuracy_score(data['madeForKids'], entire_data_preds)\n",
    "\n",
    "print('')\n",
    "print('Training Accuracy: {:.4}%'.format(training_accuracy * 100))\n",
    "print('Confusion Matrix (True Negative / False Positive / False Negative / True Positive):')\n",
    "print(confusion_matrix(y_train, grid_ab.predict(X_train)))\n",
    "print('Validation accuracy: {:.4}%'.format(test_accuracy * 100))\n",
    "print('Confusion Matrix (True Negative / False Positive / False Negative / True Positive):')\n",
    "print(confusion_matrix(y_test, grid_ab.predict(X_test)))\n",
    "print('Entire Data Accuracy: {:.4}%'.format(entire_data_accuracy * 100))\n",
    "print('Confusion Matrix (True Negative / False Positive / False Negative / True Positive):')\n",
    "print(confusion_matrix(data['madeForKids'], entire_data_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search found the following optimal parameters: \n",
      "alpha: 0.001\n",
      "\n",
      "Training Accuracy: 99.69%\n",
      "Confusion Matrix (True Negative / False Positive / False Negative / True Positive):\n",
      "[[159   0]\n",
      " [  1 158]]\n",
      "Validation accuracy: 94.34%\n",
      "Confusion Matrix (True Negative / False Positive / False Negative / True Positive):\n",
      "[[51  2]\n",
      " [ 4 49]]\n",
      "Entire Data Accuracy: 94.98%\n",
      "Confusion Matrix (True Negative / False Positive / False Negative / True Positive):\n",
      "[[3903  212]\n",
      " [   5  207]]\n"
     ]
    }
   ],
   "source": [
    "# Model 3: Naive Bayes\n",
    "\n",
    "nb = MultinomialNB()\n",
    "grid_nb = GridSearchCV(nb, {'alpha': [0.001, 0.005, 0.01]}, scoring='accuracy', cv = 5, n_jobs = 1)\n",
    "grid_nb.fit(X_train, y_train)\n",
    "\n",
    "best_parameters = grid_nb.best_params_\n",
    "\n",
    "print('Grid Search found the following optimal parameters: ')\n",
    "for param_name in sorted(best_parameters.keys()):\n",
    "    print('%s: %r' % (param_name, best_parameters[param_name]))\n",
    "\n",
    "training_accuracy = accuracy_score(y_train, grid_nb.predict(X_train))\n",
    "test_accuracy = accuracy_score(y_test, grid_nb.predict(X_test))\n",
    "entire_data_preds = grid_nb.predict(features[list(X_train.columns)])\n",
    "entire_data_accuracy = accuracy_score(data['madeForKids'], entire_data_preds)\n",
    "\n",
    "print('')\n",
    "print('Training Accuracy: {:.4}%'.format(training_accuracy * 100))\n",
    "print('Confusion Matrix (True Negative / False Positive / False Negative / True Positive):')\n",
    "print(confusion_matrix(y_train, grid_nb.predict(X_train)))\n",
    "print('Validation accuracy: {:.4}%'.format(test_accuracy * 100))\n",
    "print('Confusion Matrix (True Negative / False Positive / False Negative / True Positive):')\n",
    "print(confusion_matrix(y_test, grid_nb.predict(X_test)))\n",
    "print('Entire Data Accuracy: {:.4}%'.format(entire_data_accuracy * 100))\n",
    "print('Confusion Matrix (True Negative / False Positive / False Negative / True Positive):')\n",
    "print(confusion_matrix(data['madeForKids'], entire_data_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search found the following optimal parameters: \n",
      "learning_rate: 0.5\n",
      "\n",
      "Training Accuracy: 97.8%\n",
      "Confusion Matrix (True Negative / False Positive / False Negative / True Positive):\n",
      "[[157   2]\n",
      " [  5 154]]\n",
      "Validation accuracy: 88.68%\n",
      "Confusion Matrix (True Negative / False Positive / False Negative / True Positive):\n",
      "[[49  4]\n",
      " [ 8 45]]\n",
      "Entire Data Accuracy: 85.53%\n",
      "Confusion Matrix (True Negative / False Positive / False Negative / True Positive):\n",
      "[[3502  613]\n",
      " [  13  199]]\n"
     ]
    }
   ],
   "source": [
    "# Model 4: XGBoost\n",
    "\n",
    "xg = XGBClassifier()\n",
    "grid_xg = GridSearchCV(xg, {'learning_rate': [0.5, 0.55, 0.6]}, scoring='accuracy', cv = 5, n_jobs = 1)\n",
    "grid_xg.fit(X_train, y_train)\n",
    "\n",
    "best_parameters = grid_xg.best_params_\n",
    "\n",
    "print('Grid Search found the following optimal parameters: ')\n",
    "for param_name in sorted(best_parameters.keys()):\n",
    "    print('%s: %r' % (param_name, best_parameters[param_name]))\n",
    "\n",
    "training_accuracy = accuracy_score(y_train, grid_xg.predict(X_train))\n",
    "test_accuracy = accuracy_score(y_test, grid_xg.predict(X_test))\n",
    "entire_data_preds = grid_xg.predict(features[list(X_train.columns)])\n",
    "entire_data_accuracy = accuracy_score(data['madeForKids'], entire_data_preds)\n",
    "\n",
    "print('')\n",
    "print('Training Accuracy: {:.4}%'.format(training_accuracy * 100))\n",
    "print('Confusion Matrix (True Negative / False Positive / False Negative / True Positive):')\n",
    "print(confusion_matrix(y_train, grid_xg.predict(X_train)))\n",
    "print('Validation accuracy: {:.4}%'.format(test_accuracy * 100))\n",
    "print('Confusion Matrix (True Negative / False Positive / False Negative / True Positive):')\n",
    "print(confusion_matrix(y_test, grid_xg.predict(X_test)))\n",
    "print('Entire Data Accuracy: {:.4}%'.format(entire_data_accuracy * 100))\n",
    "print('Confusion Matrix (True Negative / False Positive / False Negative / True Positive):')\n",
    "print(confusion_matrix(data['madeForKids'], entire_data_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the information printed above, I pick the model using Naive Bayes. It has the highest validation accuracy and entire data accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Additional Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I run some additional tests to see if the best model using Naive Bayes is indeed able to predict the type of videos accurately. For this exercise, I pulled top 4 videos made for kids in a search result and saw if the model predicts the right class. I also add predictions from Gradient Boosting for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=138378474782-vjv918m0tiptts1i8hk4ip7jkqjh7sdr.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8080%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fyoutube.force-ssl&state=mmSxlIviem1nuTcefZqsl4zwr73ITu&access_type=offline\n"
     ]
    }
   ],
   "source": [
    "# Get credentials and create an API client\n",
    "\n",
    "scopes = [\"https://www.googleapis.com/auth/youtube.force-ssl\"]\n",
    "\n",
    "os.environ[\"OAUTHLIB_INSECURE_TRANSPORT\"] = \"1\"\n",
    "\n",
    "api_service_name = \"youtube\"\n",
    "api_version = \"v3\"\n",
    "client_secrets_file = \"YOUR_CLIENT_SECRET_FILE.json\"\n",
    "\n",
    "flow = google_auth_oauthlib.flow.InstalledAppFlow.from_client_secrets_file(client_secrets_file, scopes)\n",
    "credentials = flow.run_local_server()\n",
    "youtube = googleapiclient.discovery.build(api_service_name, api_version, credentials = credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video Title: Yummy Ice Cream for Kids | Cartoon for toddlers | Funny Videos for Babies | BST Live\n",
      "Prediction (Gradient Boosting): True\n",
      "Prediction (Naive Bayes): True\n",
      "Actual: True\n"
     ]
    }
   ],
   "source": [
    "# Video 1\n",
    "\n",
    "test = youtube.videos().list(part = 'id,snippet,status', id = 'tSalBQo4Q38').execute()\n",
    "test_text = test['items'][0]['snippet']['title'] + ' ' + \\\n",
    "' '.join(test['items'][0]['snippet']['description'].split())\n",
    "test_text = pd.DataFrame.from_dict([bag_it(test_text.lower())])\n",
    "test_text = pd.DataFrame(pd.concat([X_train, test_text], axis = 0).iloc[-1, :len(X_train.columns)].replace(np.nan, 0))\n",
    "\n",
    "print('Video Title: Yummy Ice Cream for Kids | Cartoon for toddlers | Funny Videos for Babies | BST Live')\n",
    "print('Prediction (Gradient Boosting): {}'.format(grid_gb.predict(test_text.transpose())[0]))\n",
    "print('Prediction (Naive Bayes): {}'.format(grid_nb.predict(test_text.transpose())[0]))\n",
    "print('Actual: {}'.format(test['items'][0]['status']['madeForKids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video Title: Best Learning Video for Toddlers Learn Colors with Crayon Surprises!\n",
      "Prediction (Gradient Boosting): True\n",
      "Prediction (Naive Bayes): True\n",
      "Actual: True\n"
     ]
    }
   ],
   "source": [
    "# Video 2\n",
    "\n",
    "test = youtube.videos().list(part = 'id,snippet,status', id = '9_WBQISVHnw').execute()\n",
    "test_text = test['items'][0]['snippet']['title'] + ' ' + \\\n",
    "' '.join(test['items'][0]['snippet']['description'].split()) + \\\n",
    "' '.join(test['items'][0]['snippet']['tags'])\n",
    "test_text = pd.DataFrame.from_dict([bag_it(test_text.lower())])\n",
    "test_text = pd.DataFrame(pd.concat([X_train, test_text], axis = 0).iloc[-1, :len(X_train.columns)].replace(np.nan, 0))\n",
    "\n",
    "print('Video Title: Best Learning Video for Toddlers Learn Colors with Crayon Surprises!')\n",
    "print('Prediction (Gradient Boosting): {}'.format(grid_gb.predict(test_text.transpose())[0]))\n",
    "print('Prediction (Naive Bayes): {}'.format(grid_nb.predict(test_text.transpose())[0]))\n",
    "print('Actual: {}'.format(test['items'][0]['status']['madeForKids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video Title: Vlad and Niki The best stories for kids | 1 Hour Video\n",
      "Prediction (Gradient Boosting): True\n",
      "Prediction (Naive Bayes): True\n",
      "Actual: True\n"
     ]
    }
   ],
   "source": [
    "# Video 3\n",
    "\n",
    "test = youtube.videos().list(part = 'id,snippet,status', id = 'Wyd9cYmLZ10').execute()\n",
    "test_text = test['items'][0]['snippet']['title'] + ' ' + \\\n",
    "' '.join(test['items'][0]['snippet']['description'].split()) + \\\n",
    "' '.join(test['items'][0]['snippet']['tags'])\n",
    "test_text = pd.DataFrame.from_dict([bag_it(test_text.lower())])\n",
    "test_text = pd.DataFrame(pd.concat([X_train, test_text], axis = 0).iloc[-1, :len(X_train.columns)].replace(np.nan, 0))\n",
    "\n",
    "print('Video Title: Vlad and Niki The best stories for kids | 1 Hour Video')\n",
    "print('Prediction (Gradient Boosting): {}'.format(grid_gb.predict(test_text.transpose())[0]))\n",
    "print('Prediction (Naive Bayes): {}'.format(grid_nb.predict(test_text.transpose())[0]))\n",
    "print('Actual: {}'.format(test['items'][0]['status']['madeForKids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video Title: Nastya and Dad best stories for kids! Video collection for the whole family\n",
      "Prediction (Gradient Boosting): True\n",
      "Prediction (Naive Bayes): True\n",
      "Actual: True\n"
     ]
    }
   ],
   "source": [
    "# Video 4\n",
    "\n",
    "test = youtube.videos().list(part = 'id,snippet,status', id = 'UQyHkp8-B6E').execute()\n",
    "test_text = test['items'][0]['snippet']['title'] + ' ' + \\\n",
    "' '.join(test['items'][0]['snippet']['description'].split()) + \\\n",
    "' '.join(test['items'][0]['snippet']['tags'])\n",
    "test_text = pd.DataFrame.from_dict([bag_it(test_text.lower())])\n",
    "test_text = pd.DataFrame(pd.concat([X_train, test_text], axis = 0).iloc[-1, :len(X_train.columns)].replace(np.nan, 0))\n",
    "\n",
    "print('Video Title: Nastya and Dad best stories for kids! Video collection for the whole family')\n",
    "print('Prediction (Gradient Boosting): {}'.format(grid_gb.predict(test_text.transpose())[0]))\n",
    "print('Prediction (Naive Bayes): {}'.format(grid_nb.predict(test_text.transpose())[0]))\n",
    "print('Actual: {}'.format(test['items'][0]['status']['madeForKids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Next Steps\n",
    "\n",
    "1. Include more videos for training, especially the videos made for kids. At the moment, the models appear to train better or worse depending on the sample it draws from the majority class as only the same number of videos as the ones made for kids can be sampled. \n",
    "2. Try different kinds of parameters to improve the models. Besides alphas and learning rates, there are other parameters such as base estimators, max depths, etc. Tuning such parameters can potentially improve the models.\n",
    "3. Try different data set-ups to improve the models. Some models may work better even with slightly imbalanced data, which allows using more data from the majority class. Also, for Naive Bayes models, it may potentially make sense to remove correlated words to comply with the assumption of independence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. Conclusion\n",
    "\n",
    "In this project, I trained classifiers to identify YouTube videos made for kids. A Naive Bayes classifier using video titles, descriptions, and tags for classification led to a validation accuracy of 95%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
